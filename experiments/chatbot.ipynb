{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's develop a chatbot based on groq llms using langchain and deploy it on haggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = ChatGroq(\n",
    "    groq_api_key = os.environ[\"GROQ_API_KEY\"],\n",
    "    model_name = \"llama-3.1-8b-instant\", #Check groq website for other llm ids\n",
    "    temperature = 0.6 \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates nd langchain chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(input_variables = [\"word\"],\n",
    "                                 template = \"What are 5 synonyms of word {word}\")\n",
    "prompt_template.format(word = \"impossible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = groq_llm, prompt = prompt_template)\n",
    "synonyme_response = chain.run(\"impossible\")\n",
    "print (synonyme_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (chain.run(\"possible\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining multiple chains using simple sequential chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = ChatGroq(\n",
    "    groq_api_key = os.environ[\"GROQ_API_KEY\"],\n",
    "    model_name = \"llama-3.1-8b-instant\",\n",
    "    temperature = 0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = PromptTemplate(input_variables =[\"word\"], \n",
    "                             template = \"What are one synonym of word {word}, return the synonyme and nothing else\" )\n",
    "first_chain = LLMChain(llm = groq_llm, prompt = first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_chain.run(word = \"impossible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(input_variables = [\"synonyme\"],\n",
    "                                template = \"What is the translation of this word {synonyme} in french\")\n",
    "second_chain = LLMChain(llm = groq_llm, prompt = second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(second_chain.run(synonyme = \"Infeasible\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequantial chain \n",
    "simple_sequence_chain = SimpleSequentialChain(chains = [first_chain, second_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sequence_chain.run(\"nice\")\n",
    "print (output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential chains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_template = PromptTemplate(input_variables = [\"proverb\"], \n",
    "                                template = \"Complete this proverb {proverb}, return the complete proverb only nothin more.\")\n",
    "first_chain = LLMChain(llm = groq_llm, prompt = first_template, output_key =\"proverb_completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (first_chain.run(\"He who plays with the sword\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_template = PromptTemplate (input_variables = [\"proverb_completed\"], \n",
    "                                  template = \"\"\"\n",
    "                                                Translate this proverbe <{proverb_completed}> to french, \n",
    "                                                return only one translation and nothing more.\n",
    "                                             \"\"\")\n",
    "second_chain = LLMChain(llm = groq_llm, prompt = second_template, output_key = \"translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (second_chain.run(\"He who plays with the sword, must be prepared to be wounded.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_chain = SequentialChain(chains = [first_chain, second_chain],\n",
    "                input_variables = [\"proverb\"],\n",
    "                output_variables = [\"proverb_completed\", \"translation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_chain({\"proverb\": \"He who plays with the sowrd\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat models, System messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [(SystemMessage(\"You are an expert in proverbs, user will give you one theme, you have to present 3 proverbs that corresponds to that theme\")),\n",
    "            (HumanMessage(\"Courage\"))]\n",
    "output = groq_llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph, MessagesState\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "groq_llm = ChatGroq(\n",
    "    groq_api_key = os.environ[\"GROQ_API_KEY\"],\n",
    "    model_name = \"llama-3.1-8b-instant\", #Check groq website for other llm ids\n",
    "    temperature = 0.6 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Worflow \n",
    "workflow = StateGraph(state_schema = MessagesState)\n",
    "#define how to call the model \n",
    "def call_model(state: MessagesState):\n",
    "    response = groq_llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "#node and edges\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "#Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set configuration\n",
    "config_1 = {\"configurable\": {\"thread_id\": \"chat_1\"}}\n",
    "config_2 = {\"configurable\": {\"thread_id\": \"chat_2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_app(query : str, config):\n",
    "    response = app.invoke({\"messages\": [HumanMessage(query)]}, config)\n",
    "    return response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"Hello, it is 4 o clock pm, here in morocco, what time could be in new york, America ?\"\n",
    "invoke_app(query_1, config_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"What would be the time in morocco in 2 hours, give the time only, nothing more.\"\n",
    "invoke_app(query_2, config_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_app(query_2, config_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
