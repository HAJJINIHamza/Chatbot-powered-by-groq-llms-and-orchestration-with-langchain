{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's develop a chatbot based on groq llms using langchain and deploy it on haggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = ChatGroq(\n",
    "    groq_api_key = os.environ[\"GROQ_API_KEY\"],\n",
    "    model_name = \"llama-3.1-8b-instant\", #Check groq website for other llm ids\n",
    "    temperature = 0.6 \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates nd langchain chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are 5 synonyms of word impossible'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(input_variables = [\"word\"],\n",
    "                                 template = \"What are 5 synonyms of word {word}\")\n",
    "prompt_template.format(word = \"impossible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 synonyms for the word \"impossible\":\n",
      "\n",
      "1. Unfeasible\n",
      "2. Inconceivable\n",
      "3. Unthinkable\n",
      "4. Unachievable\n",
      "5. Unattainable\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm = groq_llm, prompt = prompt_template)\n",
    "synonyme_response = chain.run(\"impossible\")\n",
    "print (synonyme_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 synonyms of the word \"possible\":\n",
      "\n",
      "1. Feasible\n",
      "2. Probable\n",
      "3. Viable\n",
      "4. Plausible\n",
      "5. Likely\n"
     ]
    }
   ],
   "source": [
    "print (chain.run(\"possible\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining multiple chains using simple sequential chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = ChatGroq(\n",
    "    groq_api_key = os.environ[\"GROQ_API_KEY\"],\n",
    "    model_name = \"llama-3.1-8b-instant\",\n",
    "    temperature = 0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = PromptTemplate(input_variables =[\"word\"], \n",
    "                             template = \"What are one synonym of word {word}, return the synonyme and nothing else\" )\n",
    "first_chain = LLMChain(llm = groq_llm, prompt = first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Infeasible'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_chain.run(word = \"impossible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(input_variables = [\"synonyme\"],\n",
    "                                template = \"What is the translation of this word {synonyme} in french\")\n",
    "second_chain = LLMChain(llm = groq_llm, prompt = second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of the word \"Infeasible\" in French is \"Impossible à réaliser\" or \"Inexécutable\". However, a more common and idiomatic translation would be \"Inabordable\" or \"Non réalisable\".\n",
      "\n",
      "Alternatively, you can also use the word \"Iréalisable\" which is more literal and direct translation of the word \"Infeasible\".\n",
      "\n",
      "It's worth noting that the context in which the word is used can affect the choice of translation.\n"
     ]
    }
   ],
   "source": [
    "print(second_chain.run(synonyme = \"Infeasible\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequantial chain \n",
    "simple_sequence_chain = SimpleSequentialChain(chains = [first_chain, second_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of the word \"Friendly\" in French is:\n",
      "\n",
      "- Amical (masculine)\n",
      "- Amicale (feminine)\n",
      "\n",
      "However, if you're referring to a friendly atmosphere or a friendly place, you can also use:\n",
      "\n",
      "- Chaleureux (warm, welcoming)\n",
      "- Accueillant (welcoming, hospitable)\n",
      "- Amical (friendly, in a general sense)\n",
      "\n",
      "If you're talking about a friendly game or competition, you can use:\n",
      "\n",
      "- Amical (friendly, in a sporting context)\n",
      "\n",
      "Keep in mind that the context and the situation will influence the correct translation and usage of the word.\n"
     ]
    }
   ],
   "source": [
    "output = sequence_chain.run(\"nice\")\n",
    "print (output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential chains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_template = PromptTemplate(input_variables = [\"proverb\"], \n",
    "                                template = \"Complete this proverb {proverb}, return the complete proverb only nothin more.\")\n",
    "first_chain = LLMChain(llm = groq_llm, prompt = first_template, output_key =\"proverb_completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He who plays with the sword, must be prepared to be wounded.\n"
     ]
    }
   ],
   "source": [
    "print (first_chain.run(\"He who plays with the sword\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_template = PromptTemplate (input_variables = [\"proverb_completed\"], \n",
    "                                  template = \"\"\"\n",
    "                                                Translate this proverbe <{proverb_completed}> to french, \n",
    "                                                return only one translation and nothing more.\n",
    "                                             \"\"\")\n",
    "second_chain = LLMChain(llm = groq_llm, prompt = second_template, output_key = \"translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"He who plays with the sword, must be prepared to be wounded\" in French is:\n",
      "\n",
      "\"Qui joue avec l'épée, doit être prêt à se faire blesser.\"\n",
      "\n",
      "However, a more idiomatic translation would be:\n",
      "\n",
      "\"Qui se fait l'amitié de l'épée, doit se faire la guerre.\"\n",
      "\n",
      "This translation uses a more poetic and idiomatic expression, which conveys the same idea as the original phrase.\n"
     ]
    }
   ],
   "source": [
    "print (second_chain.run(\"He who plays with the sword, must be prepared to be wounded.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_chain = SequentialChain(chains = [first_chain, second_chain],\n",
    "                input_variables = [\"proverb\"],\n",
    "                output_variables = [\"proverb_completed\", \"translation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proverb': 'He who plays with the sowrd',\n",
       " 'proverb_completed': 'He who plays with the sword, gets cut.',\n",
       " 'translation': '\"Qui se livre à l\\'épée, se fait couper.\"'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential_chain({\"proverb\": \"He who plays with the sowrd\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat models, System messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three proverbs that correspond to the theme of \"Courage\":\n",
      "\n",
      "1. **\"Courage is not the absence of fear, but rather the judgment that something else is more important than fear.\"** - Ambrose Redmoon\n",
      "\n",
      "This proverb emphasizes the importance of courage in the face of fear. It suggests that true courage involves prioritizing what's important over fear, rather than simply ignoring it.\n",
      "\n",
      "2. **\"Fall seven times, stand up eight.\"** - Japanese proverb\n",
      "\n",
      "This proverb conveys the idea that courage involves perseverance and resilience in the face of adversity. It suggests that even when we face setbacks, we should keep getting back up and trying again.\n",
      "\n",
      "3. **\"The lion is tamed not by a whip, but by a gentle voice.\"** - Latin proverb\n",
      "\n",
      "This proverb suggests that courage can be achieved through gentle persuasion and encouragement, rather than through force or coercion. It emphasizes the importance of kindness and compassion in building courage and confidence.\n",
      "\n",
      "I hope these proverbs inspire you to be courageous!\n"
     ]
    }
   ],
   "source": [
    "messages = [(SystemMessage(\"You are an expert in proverbs, user will give you one theme, you have to present 3 proverbs that corresponds to that theme\")),\n",
    "            (HumanMessage(\"Courage\"))]\n",
    "output = groq_llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph, MessagesState\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "groq_llm = ChatGroq(\n",
    "    groq_api_key = os.environ[\"GROQ_API_KEY\"],\n",
    "    model_name = \"llama-3.1-8b-instant\", #Check groq website for other llm ids\n",
    "    temperature = 0.6 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Worflow \n",
    "workflow = StateGraph(state_schema = MessagesState)\n",
    "#define how to call the model \n",
    "def call_model(state: MessagesState):\n",
    "    response = groq_llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "#node and edges\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "#Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set configuration\n",
    "config_1 = {\"configurable\": {\"thread_id\": \"chat_1\"}}\n",
    "config_2 = {\"configurable\": {\"thread_id\": \"chat_2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_app(query : str, config):\n",
    "    response = app.invoke({\"messages\": [HumanMessage(query)]}, config)\n",
    "    return response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Since Morocco is in the Western European Time (WET) or Central European Time (CET) zone, which is UTC+0 or UTC+1, and New York is in the Eastern Time Zone (ET), which is UTC-5, we need to consider the time difference.\n",
      "\n",
      "Assuming Morocco is in UTC+0 (winter) and New York is in standard time (UTC-5), the time difference would be 5 hours. \n",
      "\n",
      "If it's 4 o'clock pm in Morocco, \n",
      "\n",
      "- It would be 11 am in New York (4 pm + 5 hours).\n",
      "\n",
      "However, Morocco is in UTC+1 (summer) from March to October and New York is in daylight saving time from March to November, so let's consider that scenario.\n",
      "\n",
      "If Morocco is in UTC+1 and New York is in daylight saving time (UTC-4), the time difference would be 6 hours.\n",
      "\n",
      "- If it's 4 o'clock pm in Morocco, it would be 10 am in New York (4 pm + 6 hours).\n",
      "\n",
      "Please note that Morocco and New York may not be in the same time zone or daylight saving time at the same time.\n"
     ]
    }
   ],
   "source": [
    "query_1 = \"Hello, it is 4 o clock pm, here in morocco, what time could be in new york, America ?\"\n",
    "invoke_app(query_1, config_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "6 pm\n"
     ]
    }
   ],
   "source": [
    "query_2 = \"What would be the time in morocco in 2 hours, give the time only, nothing more.\"\n",
    "invoke_app(query_2, config_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "UTC+0 \n",
      "\n",
      "Current time: 12:00 \n",
      "In 2 hours: 14:00\n"
     ]
    }
   ],
   "source": [
    "invoke_app(query_2, config_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "from typing import TypedDict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = ChatGroq( groq_api_key = os.environ[\"GROQ_API_KEY\"], \n",
    "                        model_name = \"llama-3.1-8b-instant\", \n",
    "                        temperature = 0.6 ) \n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "\n",
    "#Build the app using langraph \n",
    "workflow = StateGraph(state_schema = MessagesState) \n",
    "# \n",
    "def call_model(state:MessagesState): \n",
    "\tresponse = groq_llm.invoke(state[\"messages\"]) \n",
    "\treturn {\"messages\": response} \n",
    "\n",
    "#add nodes and edges \n",
    "workflow.add_edge(START, \"model\") \n",
    "workflow.add_node(\"model\", call_model) \n",
    "#Memory \n",
    "memory = MemorySaver() \n",
    "app = workflow.compile(checkpointer = memory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '111'}}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_3 = {\"configurable\" : {\"thread_id\": \"111\"}}\n",
    "config_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config type : <class 'dict'>\n",
      "config:  {'configurable': {'thread_id': '111'}}\n",
      "thread : 111\n"
     ]
    }
   ],
   "source": [
    "print(\"config type :\", type(config_3))\n",
    "print (\"config: \", config_3)\n",
    "thread_id = config_3[\"configurable\"][\"thread_id\"]\n",
    "print (\"thread :\", thread_id)\n",
    "# Retrieve prior state (conversation history) from the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={}, next=(), config={'configurable': {'thread_id': '111'}}, metadata=None, created_at=None, parent_config=None, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.get_state(config_3).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    previous_state = app.get_state(config_3)\n",
    "    messages = previous_state[\"messages\"]\n",
    "except Exception:\n",
    "    messages = []\n",
    "messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hello\"\n",
    "messages.append(HumanMessage(content=query))\n",
    "messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the app with the full message history\n",
    "response = app.invoke({\"messages\": messages}, config_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello. How can I assist you today?'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 36, 'total_tokens': 46, 'completion_time': 0.011811121, 'prompt_time': 0.00155279, 'queue_time': 0.088089074, 'total_time': 0.013363911}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5fac31c5-5d71-4375-bf20-2fcaec4ee55a-0', usage_metadata={'input_tokens': 36, 'output_tokens': 10, 'total_tokens': 46})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.get_state(config_3).values[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
