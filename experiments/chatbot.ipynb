{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's develop a chatbot based on groq llms using langchain and deploy it on haggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test wether api key is there or not\n",
    "os.environ[\"HUGGINGFACE_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = ChatGroq(\n",
    "    groq_api_key = os.environ[\"GROQ_API_KEY\"],\n",
    "    model_name = \"llama-3.1-8b-instant\", #Check groq website for other llm ids\n",
    "    temperature = 0.6 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. I can see that you've initiated a conversation. I'm here to assist you with any questions or topics you'd like to discuss. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "test_prompt = \"Hello, do you hear me ?\"\n",
    "response = groq_llm.invoke(test_prompt)\n",
    "print (response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, yes I hear you. Is there something I can help you with or would you like to chat?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test 2\n",
    "groq_llm.predict(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_t5_llm = HuggingFaceEndpoint(huggingfacehub_api_token = os.environ[\"HUGGINGFACE_API_TOKEN\"],\n",
    "                                repo_id=\"google/flan-t5-large\", \n",
    "                                task = 'text-generation',\n",
    "                                temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_t5_llm.invoke(\"hello\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates nd langchain chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are 5 synonyms of word impossible'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(input_variables = [\"word\"],\n",
    "                                 template = \"What are 5 synonyms of word {word}\")\n",
    "prompt_template.format(word = \"impossible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 synonyms for the word \"impossible\":\n",
      "\n",
      "1. Unfeasible\n",
      "2. Inconceivable\n",
      "3. Unthinkable\n",
      "4. Unachievable\n",
      "5. Unattainable\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm = groq_llm, prompt = prompt_template)\n",
    "synonyme_response = chain.run(\"impossible\")\n",
    "print (synonyme_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 synonyms of the word \"possible\":\n",
      "\n",
      "1. Feasible\n",
      "2. Probable\n",
      "3. Viable\n",
      "4. Plausible\n",
      "5. Likely\n"
     ]
    }
   ],
   "source": [
    "print (chain.run(\"possible\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining multiple chains using simple sequential chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = ChatGroq(\n",
    "    groq_api_key = os.environ[\"GROQ_API_KEY\"],\n",
    "    model_name = \"llama-3.1-8b-instant\",\n",
    "    temperature = 0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = PromptTemplate(input_variables =[\"word\"], \n",
    "                             template = \"What are one synonym of word {word}, return the synonyme and nothing else\" )\n",
    "first_chain = LLMChain(llm = groq_llm, prompt = first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Infeasible'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_chain.run(word = \"impossible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(input_variables = [\"synonyme\"],\n",
    "                                template = \"What is the translation of this word {synonyme} in french\")\n",
    "second_chain = LLMChain(llm = groq_llm, prompt = second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of the word \"Infeasible\" in French is \"Impossible à réaliser\" or \"Inexécutable\". However, a more common and idiomatic translation would be \"Inabordable\" or \"Non réalisable\".\n",
      "\n",
      "Alternatively, you can also use the word \"Iréalisable\" which is more literal and direct translation of the word \"Infeasible\".\n",
      "\n",
      "It's worth noting that the context in which the word is used can affect the choice of translation.\n"
     ]
    }
   ],
   "source": [
    "print(second_chain.run(synonyme = \"Infeasible\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequantial chain \n",
    "simple_sequence_chain = SimpleSequentialChain(chains = [first_chain, second_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of the word \"Friendly\" in French is:\n",
      "\n",
      "- Amical (masculine)\n",
      "- Amicale (feminine)\n",
      "\n",
      "However, if you're referring to a friendly atmosphere or a friendly place, you can also use:\n",
      "\n",
      "- Chaleureux (warm, welcoming)\n",
      "- Accueillant (welcoming, hospitable)\n",
      "- Amical (friendly, in a general sense)\n",
      "\n",
      "If you're talking about a friendly game or competition, you can use:\n",
      "\n",
      "- Amical (friendly, in a sporting context)\n",
      "\n",
      "Keep in mind that the context and the situation will influence the correct translation and usage of the word.\n"
     ]
    }
   ],
   "source": [
    "output = sequence_chain.run(\"nice\")\n",
    "print (output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential chains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_template = PromptTemplate(input_variables = [\"proverb\"], \n",
    "                                template = \"Complete this proverb {proverb}, return the complete proverb only nothin more.\")\n",
    "first_chain = LLMChain(llm = groq_llm, prompt = first_template, output_key =\"proverb_completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He who plays with the sword, must be prepared to be wounded.\n"
     ]
    }
   ],
   "source": [
    "print (first_chain.run(\"He who plays with the sword\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_template = PromptTemplate (input_variables = [\"proverb_completed\"], \n",
    "                                  template = \"\"\"\n",
    "                                                Translate this proverbe <{proverb_completed}> to french, \n",
    "                                                return only one translation and nothing more.\n",
    "                                             \"\"\")\n",
    "second_chain = LLMChain(llm = groq_llm, prompt = second_template, output_key = \"translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"He who plays with the sword, must be prepared to be wounded\" in French is:\n",
      "\n",
      "\"Qui joue avec l'épée, doit être prêt à se faire blesser.\"\n",
      "\n",
      "However, a more idiomatic translation would be:\n",
      "\n",
      "\"Qui se fait l'amitié de l'épée, doit se faire la guerre.\"\n",
      "\n",
      "This translation uses a more poetic and idiomatic expression, which conveys the same idea as the original phrase.\n"
     ]
    }
   ],
   "source": [
    "print (second_chain.run(\"He who plays with the sword, must be prepared to be wounded.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_chain = SequentialChain(chains = [first_chain, second_chain],\n",
    "                input_variables = [\"proverb\"],\n",
    "                output_variables = [\"proverb_completed\", \"translation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proverb': 'He who plays with the sowrd',\n",
       " 'proverb_completed': 'He who plays with the sword, gets cut.',\n",
       " 'translation': '\"Qui se livre à l\\'épée, se fait couper.\"'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential_chain({\"proverb\": \"He who plays with the sowrd\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
